{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Probabilistic Models\n",
        "**Focus:** Generative latent-variable and probabilistic manifold models forming the theoretical lineage of *The Generative Topographic Mapping (GTM)*  \n",
        "*(Bishop, Svensén, & Williams, 1998)*\n",
        "\n",
        "| **Author(s)** | **Year** | **Title** | **Venue** | **Probabilistic Model Introduced (and Connection)** |\n",
        "|----------------|-----------|------------|------------|-----------------------------------------------------|\n",
        "| **Bishop, C. M., Svensén, M., & Williams, C. K. I.** | 1998 | *The Generative Topographic Mapping (GTM)* | *Neural Computation* | Introduces **GTM**, a **generative latent-variable model** with a discrete latent prior on a grid, nonlinear mapping \\( y(x;W) \\), and isotropic Gaussian noise. Parameters are estimated via the **Expectation–Maximization (EM)** algorithm. Provides the **probabilistic counterpart to SOM**, ensuring convergence, interpretability, and density modeling. <br> *Source:* [direct.mit.edu](https://direct.mit.edu) |\n",
        "| **MacKay, D. J. C.** | 1995 | *Bayesian Neural Networks and Density Networks* | *Nuclear Instruments and Methods in Physics Research A* | Proposes **Density Networks**, probabilistic density models parameterized by neural networks. Establishes the conceptual basis for **manifold learning as density estimation**, directly influencing GTM’s **probabilistic generative formulation**. <br> *Sources:* [sciencedirect.com](https://www.sciencedirect.com), [inference.org.uk](https://inference.org.uk) |\n",
        "| **Tibshirani, R.** | 1992 | *Principal Curves Revisited* | *Statistics and Computing* | Develops a **generative EM-based Gaussian mixture formulation** of principal curves — an early **probabilistic principal-manifold** approach. GTM generalizes this by imposing a structured latent grid and explicit EM training for manifold unfolding. <br> *Source:* [Scribd](https://www.scribd.com) |\n",
        "| **Ghahramani, Z. & Hinton, G. E.** | 1997 | *The EM Algorithm for Mixtures of Factor Analyzers* | *Technical Report, University of Toronto* | Introduces **Mixtures of Factor Analyzers (MFA)** — a probabilistic model combining local linear subspaces with global density estimation. GTM is comparable as a **constrained mixture of Gaussians**, where centers follow a nonlinear manifold rather than arbitrary positions. <br> *Source:* [cs.toronto.edu](https://www.cs.toronto.edu) |\n",
        "| **Tipping, M. E. & Bishop, C. M.** | 1999 | *Probabilistic Principal Component Analysis (PPCA)* | *Journal of the Royal Statistical Society, Series B* | Establishes **PPCA**, a **maximum-likelihood and probabilistic reformulation of PCA**. Provides the **linear baseline** for probabilistic manifold models, which GTM extends to **nonlinear embeddings** via radial basis mappings. <br> *Sources:* [rss.onlinelibrary.wiley.com](https://rss.onlinelibrary.wiley.com), [di.ens.fr](https://di.ens.fr) |\n",
        "| **Luttrell, S. P.** | 1994 | *A Bayesian Analysis of Self-Organizing Maps* | *Neural Computation* | Presents a **Bayesian reinterpretation of SOM**, viewing it as a stochastic process approximating a latent distribution. Serves as a **precursor to GTM’s fully generative and EM-trainable model**, replacing heuristics with formal likelihood maximization. <br> *Source:* [direct.mit.edu](https://direct.mit.edu) |\n",
        "| **Hinton, G. E., Williams, C. K. I., & Revow, M.** | 1991 | *Adaptive Elastic Models for Hand-Printed Character Recognition* | *Advances in Neural Information Processing Systems (NIPS 4)* | Introduces **adaptive elastic models**, combining Gaussian mixtures with spline constraints for probabilistic shape modeling. Anticipates GTM’s use of **structured Gaussian mixtures** with smooth latent topology. <br> *Sources:* [cs.toronto.edu](https://www.cs.toronto.edu), [proceedings.neurips.cc](https://proceedings.neurips.cc) |\n",
        "\n",
        "---\n",
        "\n",
        "## **Summary and Connection to GTM**\n",
        "\n",
        "These works collectively form the **probabilistic and statistical foundation** upon which *Generative Topographic Mapping (GTM)* was built.  \n",
        "\n",
        "### **Conceptual Lineage**\n",
        "| **Domain** | **Representative Work(s)** | **Contribution to GTM** |\n",
        "|-------------|-----------------------------|--------------------------|\n",
        "| **Probabilistic Latent Variable Models** | Ghahramani & Hinton (1997), Tipping & Bishop (1999) | Introduced EM-based estimation and probabilistic linear manifolds (Factor Analysis, PPCA). GTM extends these to nonlinear mappings. |\n",
        "| **Bayesian and Density Modeling** | MacKay (1995), Luttrell (1994) | Provided frameworks for Bayesian density estimation and probabilistic SOM reformulation. GTM synthesizes these into a generative topographic structure. |\n",
        "| **Manifold Learning** | Tibshirani (1992), Hinton et al. (1991) | Early probabilistic manifold and spline models. GTM generalizes them into a unified EM-trained mixture framework. |\n",
        "\n",
        "---\n",
        "\n",
        "### **Integrative Insight**\n",
        "GTM unifies the key advances of probabilistic modeling:\n",
        "- **From**: heuristic topology preservation (SOM, elastic nets)  \n",
        "- **Through**: probabilistic inference (EM, Bayesian priors)  \n",
        "- **To**: a fully **generative density model** with smooth manifold geometry  \n",
        "\n",
        "Thus, Bishop et al. (1998) formalized **topographic mapping** within the broader statistical paradigm of **latent-variable modeling**, bridging neural computation and probabilistic inference.\n"
      ],
      "metadata": {
        "id": "UixCTQ7o056R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Major Probabilistic Generative Model Families and Examples\n",
        "\n",
        "| **Model Family** | **Key Characteristic / Probability Structure** | **Notable Examples** |\n",
        "|-------------------|-----------------------------------------------|----------------------|\n",
        "| **Autoregressive Models** | Factorize the joint probability distribution as a product of conditionals:  <br> $$p(x) = \\prod_i p(x_i \\mid x_{<i})$$  These models generate each component sequentially, conditioning on previously generated parts. They yield exact likelihoods and stable training. | GPT series (*GPT-1, GPT-2, GPT-3, GPT-4*) for text generation; PixelCNN and PixelRNN for images; WaveNet for audio synthesis. |\n",
        "| **Variational Autoencoders (VAEs)** | Latent-variable models that approximate the posterior \\( p(z \\mid x) \\) with an inference model \\( q(z \\mid x) \\). They optimize the **Evidence Lower Bound (ELBO):**  <br> $$\\ln p(x) \\ge \\mathbb{E}_{q(z \\mid x)}[\\ln p(x \\mid z)] - \\mathrm{KL}(q(z \\mid x) \\parallel p(z))$$  enabling scalable stochastic gradient optimization. | VAE (Kingma & Welling, 2013); β-VAE; NVAE; BIVA; VQ-VAE (discrete latent variant). |\n",
        "| **Normalizing Flows (Flow-Based Models)** | Define an **invertible and differentiable mapping** between latent and data spaces:  <br> $$x = f(z), \\quad z \\sim p(z)$$  with exact likelihood computed via the **change-of-variables formula:**  <br> $$\\ln p(x) = \\ln p(z) - \\ln\\left|\\det \\frac{df}{dz}\\right|$$  allowing exact density modeling and efficient inference. | RealNVP, Glow, NICE, FFJORD, Flow++ — used for density estimation and image synthesis. |\n",
        "| **Diffusion / Score-Based Models** | Model a **forward noising process** (data → noise) and learn the **reverse denoising process** via parameterized transitions or stochastic differential equations (SDEs). Likelihoods are approximated through variational bounds or score matching. | DDPM (Denoising Diffusion Probabilistic Models), DDIM, Stable Diffusion, Imagen, DALL·E 2 (diffusion–transformer hybrids). |\n",
        "| **Generative Adversarial Networks (GANs)** | Implicit generative framework using an adversarial game between generator \\( G(z) \\) and discriminator \\( D(x) \\). GANs lack explicit likelihoods, relying on **minimax optimization** to match distributions. | DCGAN, StyleGAN, BigGAN, CycleGAN, InfoGAN — strong sample fidelity but no explicit probability model. |\n",
        "| **Hybrid and Conditional Generative Models** | Combine multiple probabilistic paradigms or introduce conditioning for **controlled or multimodal generation**. Examples include VAE–GAN hybrids, conditional diffusion models, and flow–autoregressive compositions. | VQ-VAE + transformer prior, conditional diffusion models (e.g., text-to-image), CLIP-guided diffusion, FlowAR hybrids. |\n",
        "\n",
        "---\n",
        "\n",
        "## **Remarks and Trends**\n",
        "\n",
        "### **1. Trade-off: Tractability vs. Flexibility**\n",
        "- **Autoregressive** and **flow-based** models provide **exact likelihoods** and interpretable probabilities but can be **computationally costly** during sampling.  \n",
        "- **VAEs** and **diffusion models** relax exact likelihoods, favoring **flexibility and stability** over strict tractability.\n",
        "\n",
        "### **2. Likelihood Scaling and Sample Quality**\n",
        "- Early VAEs generated blurry outputs due to simplistic decoders and weak posteriors.  \n",
        "- **Hierarchical VAEs (NVAE, BIVA)** and **diffusion models** have since achieved **near state-of-the-art fidelity**, rivaling or surpassing GANs in perceptual realism.\n",
        "\n",
        "### **3. Conditional and Multimodal Extensions**\n",
        "- Conditioning mechanisms enable **controlled generation**:  \n",
        "  - \\( p(x \\mid c) \\) where \\( c \\) is text, class, or style.  \n",
        "  - Examples: text-to-image (DALL·E 2, Imagen), audio-to-video, and multimodal fusion models.  \n",
        "- Hybrid probabilistic architectures now allow **joint modeling** of images, audio, and language under a unified probabilistic view.\n",
        "\n",
        "### **4. Transformers and Large-Scale Modeling**\n",
        "- Transformer-based architectures underpin **modern autoregressive and diffusion models**, enabling long-range dependency modeling and **cross-modal generalization**.  \n",
        "- This unification of sequence modeling and probabilistic inference has expanded generative AI beyond individual modalities — forming the foundation for **large multimodal foundation models**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary Insight**\n",
        "\n",
        "All probabilistic generative model families can be viewed through the lens of the **data likelihood**:\n",
        "\n",
        "$$\n",
        "p(x) =\n",
        "\\begin{cases}\n",
        "\\prod_i p(x_i \\mid x_{<i}) & \\text{(Autoregressive)} \\\\\n",
        "\\int p(x \\mid z) p(z) \\, dz & \\text{(VAE, Diffusion)} \\\\\n",
        "p(z) \\left|\\det \\frac{df}{dz}\\right|^{-1} & \\text{(Flow-Based)} \\\\\n",
        "\\text{Implicit via } G(z) \\text{ and } D(x) & \\text{(GAN)}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Each class trades off **explicit likelihood computation** for **expressive power and scalability**, shaping the evolution of modern generative AI — from VAEs and flows to diffusion-based and hybrid transformer-driven architectures.\n"
      ],
      "metadata": {
        "id": "m3QZ8CX71ivQ"
      }
    }
  ]
}